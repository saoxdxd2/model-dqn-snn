"""
Noise2Noise Denoiser for Text Rendered Images

Improves text rendering quality by training on noisy variants (different font sizes,
anti-aliasing, positions) without requiring clean ground truth images.

Based on: "Noise2Noise: Learning Image Restoration without Clean Data" (NVIDIA, ICML 2018)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple
import numpy as np
from PIL import Image
from models.bitnet import BitLinear


class UNet(nn.Module):
    """
    U-Net architecture for image denoising.
    
    Encoder-decoder with skip connections for preserving spatial information.
    Lightweight version optimized for 224x224 text images.
    """
    
    def __init__(
        self,
        in_channels: int = 3,
        out_channels: int = 3,
        base_channels: int = 32,
        depth: int = 4
    ):
        """
        Args:
            in_channels: Input image channels (3 for RGB)
            out_channels: Output image channels (3 for RGB)
            base_channels: Base number of channels (doubled at each level)
            depth: Number of encoder/decoder levels
        """
        super().__init__()
        
        self.depth = depth
        
        # Encoder (downsampling)
        self.encoders = nn.ModuleList()
        self.pools = nn.ModuleList()
        
        channels = in_channels
        for i in range(depth):
            out_ch = base_channels * (2 ** i)
            self.encoders.append(self._conv_block(channels, out_ch))
            self.pools.append(nn.MaxPool2d(2))
            channels = out_ch
        
        # Bottleneck
        self.bottleneck = self._conv_block(channels, channels * 2)
        
        # Decoder (upsampling)
        self.decoders = nn.ModuleList()
        self.upsamples = nn.ModuleList()
        
        channels = channels * 2
        for i in range(depth):
            out_ch = channels // 2
            self.upsamples.append(nn.ConvTranspose2d(channels, out_ch, kernel_size=2, stride=2))
            # Skip connection doubles channels
            self.decoders.append(self._conv_block(channels, out_ch))
            channels = out_ch
        
        # Final output
        self.final = nn.Conv2d(channels, out_channels, kernel_size=1)
    
    def _conv_block(self, in_ch: int, out_ch: int) -> nn.Module:
        """Double convolution block with ReLU."""
        return nn.Sequential(
            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Input tensor [B, C, H, W]
        
        Returns:
            Denoised tensor [B, C, H, W]
        """
        # Encoder with skip connections
        skips = []
        for encoder, pool in zip(self.encoders, self.pools):
            x = encoder(x)
            skips.append(x)
            x = pool(x)
        
        # Bottleneck
        x = self.bottleneck(x)
        
        # Decoder with skip connections
        skips = skips[::-1]  # Reverse for decoding
        for decoder, upsample, skip in zip(self.decoders, self.upsamples, skips):
            x = upsample(x)
            # Concatenate skip connection
            x = torch.cat([x, skip], dim=1)
            x = decoder(x)
        
        # Final layer
        x = self.final(x)
        return x


class Noise2NoiseDenoiser:
    """
    Noise2Noise denoiser for text rendered images.
    
    Features:
    - Train on pairs of noisy variants (no clean data needed)
    - Lightweight U-Net architecture
    - Fast inference for real-time denoising
    - GPU/CPU support
    """
    
    def __init__(
        self,
        model_path: Optional[str] = None,
        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
    ):
        """
        Args:
            model_path: Path to pretrained model weights
            device: 'cuda' or 'cpu'
        """
        self.device = device
        self.model = UNet(
            in_channels=3,
            out_channels=3,
            base_channels=32,
            depth=4
        ).to(device)
        
        if model_path is not None:
            self.load(model_path)
        
        self.model.eval()
    
    def denoise(self, image: np.ndarray) -> np.ndarray:
        """
        Denoise a single image.
        
        Args:
            image: Numpy array [H, W, 3] in range [0, 255] (RGB)
        
        Returns:
            Denoised image [H, W, 3] in range [0, 255] (RGB)
        """
        # Normalize to [0, 1]
        img_tensor = torch.from_numpy(image).float() / 255.0
        
        # HWC -> CHW
        img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0)
        img_tensor = img_tensor.to(self.device)
        
        # Inference
        with torch.no_grad():
            denoised = self.model(img_tensor)
        
        # CHW -> HWC, denormalize
        denoised = denoised.squeeze(0).permute(1, 2, 0)
        denoised = (denoised.cpu().numpy() * 255.0).clip(0, 255).astype(np.uint8)
        
        return denoised
    
    def denoise_batch(self, images: np.ndarray) -> np.ndarray:
        """
        Denoise a batch of images.
        
        Args:
            images: Numpy array [B, H, W, 3] in range [0, 255]
        
        Returns:
            Denoised images [B, H, W, 3] in range [0, 255]
        """
        # Normalize to [0, 1]
        img_tensor = torch.from_numpy(images).float() / 255.0
        
        # BHWC -> BCHW
        img_tensor = img_tensor.permute(0, 3, 1, 2).to(self.device)
        
        # Inference
        with torch.no_grad():
            denoised = self.model(img_tensor)
        
        # BCHW -> BHWC, denormalize
        denoised = denoised.permute(0, 2, 3, 1)
        denoised = (denoised.cpu().numpy() * 255.0).clip(0, 255).astype(np.uint8)
        
        return denoised
    
    def train_step(
        self,
        noisy1: torch.Tensor,
        noisy2: torch.Tensor,
        optimizer: torch.optim.Optimizer
    ) -> float:
        """
        Single training step using Noise2Noise paradigm.
        
        Args:
            noisy1: First noisy variant [B, C, H, W] in [0, 1]
            noisy2: Second noisy variant [B, C, H, W] in [0, 1] (target)
            optimizer: PyTorch optimizer
        
        Returns:
            Loss value
        """
        self.model.train()
        
        noisy1 = noisy1.to(self.device)
        noisy2 = noisy2.to(self.device)
        
        # Forward pass
        pred = self.model(noisy1)
        
        # Noise2Noise loss: predict noisy2 from noisy1
        loss = F.mse_loss(pred, noisy2)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        return loss.item()
    
    def save(self, path: str):
        """Save model weights."""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'architecture': {
                'in_channels': 3,
                'out_channels': 3,
                'base_channels': 32,
                'depth': 4
            }
        }, path)
        print(f"Model saved to {path}")
    
    def load(self, path: str):
        """Load model weights."""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        print(f"Model loaded from {path}")


class NoisyVariantGenerator:
    """
    Generate noisy variants of text renders for Noise2Noise training.
    
    Noise sources:
    - Font size variation (Â±1-2 pixels)
    - Position jitter (Â±1-3 pixels)
    - Anti-aliasing toggle
    - Slight rotation (Â±1 degree)
    - Background shade variation
    """
    
    def __init__(self, text_renderer):
        """
        Args:
            text_renderer: TextRenderer instance
        """
        self.base_renderer = text_renderer
    
    def generate_variant(
        self,
        text: str,
        variant_type: str = 'mixed'
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate two noisy variants of the same text.
        
        Args:
            text: Text to render
            variant_type: 'font_size', 'position', 'mixed'
        
        Returns:
            (variant1, variant2) as numpy arrays [H, W, 3]
        """
        from PIL import Image, ImageDraw, ImageFont
        import random
        
        # Original parameters
        width = self.base_renderer.width
        height = self.base_renderer.height
        base_font_size = self.base_renderer.font_size
        
        # Variant 1: Slight modifications
        if variant_type in ['font_size', 'mixed']:
            font_size_1 = base_font_size + random.choice([-1, 0, 1])
        else:
            font_size_1 = base_font_size
        
        if variant_type in ['position', 'mixed']:
            offset_x_1 = random.randint(-2, 2)
            offset_y_1 = random.randint(-2, 2)
        else:
            offset_x_1 = 0
            offset_y_1 = 0
        
        # Variant 2: Different modifications
        if variant_type in ['font_size', 'mixed']:
            font_size_2 = base_font_size + random.choice([-1, 0, 1])
        else:
            font_size_2 = base_font_size
        
        if variant_type in ['position', 'mixed']:
            offset_x_2 = random.randint(-2, 2)
            offset_y_2 = random.randint(-2, 2)
        else:
            offset_x_2 = 0
            offset_y_2 = 0
        
        # Render both variants
        from models.text_renderer import TextRenderer
        
        renderer1 = TextRenderer(
            width=width,
            height=height,
            font_size=font_size_1
        )
        renderer2 = TextRenderer(
            width=width,
            height=height,
            font_size=font_size_2
        )
        
        img1 = renderer1.render_plain_text(text)
        img2 = renderer2.render_plain_text(text)
        
        # Apply position jitter by cropping/padding
        if offset_x_1 != 0 or offset_y_1 != 0:
            img1 = self._apply_jitter(img1, offset_x_1, offset_y_1)
        if offset_x_2 != 0 or offset_y_2 != 0:
            img2 = self._apply_jitter(img2, offset_x_2, offset_y_2)
        
        # Convert to numpy
        arr1 = np.array(img1)
        arr2 = np.array(img2)
        
        return arr1, arr2
    
    def _apply_jitter(self, img: Image.Image, dx: int, dy: int) -> Image.Image:
        """Apply position jitter by shifting image."""
        from PIL import Image
        
        width, height = img.size
        shifted = Image.new('RGB', (width, height), color='white')
        shifted.paste(img, (dx, dy))
        return shifted


class AdaptiveNoiseGenerator(nn.Module):
    """
    SEAL-inspired adaptive noise strategy generator.
    
    Learns optimal noise parameters (font size, position jitter, denoising strength)
    based on text characteristics to maximize downstream TRM performance.
    
    Integration with SEAL framework:
    - Generates "self-edits" (noise parameters) for each text
    - Uses RL to learn what noise helps TRM most
    - Adapts strategy based on text type (code vs prose)
    """
    
    def __init__(self, input_dim: int = 768, hidden_dim: int = 256):
        """
        Args:
            input_dim: Dimension of text embeddings (768 for BERT-like)
            hidden_dim: Hidden layer dimension
        """
        super().__init__()
        
        self.encoder = nn.Sequential(
            BitLinear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            BitLinear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Predict noise parameters ("self-edit" generation)
        self.font_size_head = BitLinear(hidden_dim, 7)  # -3 to +3
        self.position_x_head = BitLinear(hidden_dim, 11)  # -5 to +5
        self.position_y_head = BitLinear(hidden_dim, 11)  # -5 to +5
        self.strength_head = BitLinear(hidden_dim, 1)  # 0.0 to 1.0 (denoising strength)
        
    def forward(self, text_features: torch.Tensor) -> dict:
        """
        Generate noise parameters for given text.
        
        Args:
            text_features: [B, input_dim] text embeddings
        
        Returns:
            Dict with logits for each parameter
        """
        h = self.encoder(text_features)
        
        return {
            'font_size_logits': self.font_size_head(h),
            'position_x_logits': self.position_x_head(h),
            'position_y_logits': self.position_y_head(h),
            'denoising_strength': torch.sigmoid(self.strength_head(h))
        }
    
    def sample_noise_params(self, text_features: torch.Tensor) -> dict:
        """
        Sample noise parameters from predicted distribution.
        
        Args:
            text_features: [B, input_dim]
        
        Returns:
            Dict with sampled parameters
        """
        preds = self.forward(text_features)
        
        # Sample from categorical distributions
        font_size_idx = torch.multinomial(
            F.softmax(preds['font_size_logits'], dim=-1), 1
        ).squeeze(-1)
        font_size_delta = font_size_idx - 3  # -3 to +3
        
        position_x_idx = torch.multinomial(
            F.softmax(preds['position_x_logits'], dim=-1), 1
        ).squeeze(-1)
        position_jitter_x = position_x_idx - 5  # -5 to +5
        
        position_y_idx = torch.multinomial(
            F.softmax(preds['position_y_logits'], dim=-1), 1
        ).squeeze(-1)
        position_jitter_y = position_y_idx - 5
        
        return {
            'font_size_delta': font_size_delta.cpu().numpy(),
            'position_jitter_x': position_jitter_x.cpu().numpy(),
            'position_jitter_y': position_jitter_y.cpu().numpy(),
            'denoising_strength': preds['denoising_strength'].squeeze(-1).cpu().numpy()
        }


class SEALNoise2Noise:
    """
    Self-Adapting Noise2Noise with SEAL framework.
    
    Closed-loop system:
    1. AdaptiveNoiseGenerator proposes noise params
    2. NoisyVariantGenerator uses these params
    3. Noise2NoiseDenoiser denoises with predicted strength
    4. TRM evaluates result (reward signal)
    5. RL updates AdaptiveNoiseGenerator
    
    Benefits:
    - Learns optimal noise for each text type
    - Adapts denoising strength dynamically
    - Improves based on downstream TRM performance
    """
    
    def __init__(
        self,
        denoiser_path: Optional[str] = None,
        adaptive_gen_path: Optional[str] = None,
        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
    ):
        self.device = device
        
        # Core denoiser
        self.denoiser = Noise2NoiseDenoiser(model_path=denoiser_path, device=device)
        
        # Adaptive noise generator (SEAL component)
        self.noise_generator = AdaptiveNoiseGenerator().to(device)
        
        if adaptive_gen_path:
            checkpoint = torch.load(adaptive_gen_path, map_location=device)
            self.noise_generator.load_state_dict(checkpoint['model_state_dict'])
            print(f"âœ“ Adaptive noise generator loaded from {adaptive_gen_path}")
        
        # RL tracking for SEAL
        self.strategy_rewards = []  # Track (params, reward) pairs
    
    def denoise_adaptive(
        self,
        image: np.ndarray,
        text_features: Optional[torch.Tensor] = None,
        denoising_strength: float = 1.0
    ) -> np.ndarray:
        """
        Denoise with adaptive strength based on text features.
        
        Args:
            image: [H, W, 3] RGB image
            text_features: [1, 768] text embedding (optional)
            denoising_strength: Manual strength override
        
        Returns:
            Denoised image [H, W, 3]
        """
        # Get adaptive denoising strength
        if text_features is not None:
            with torch.no_grad():
                params = self.noise_generator.sample_noise_params(text_features)
                denoising_strength = params['denoising_strength'][0]
        
        # Denoise
        denoised = self.denoiser.denoise(image)
        
        # Blend based on strength
        if denoising_strength < 1.0:
            # Partial denoising
            denoised = (denoised * denoising_strength + 
                       image * (1 - denoising_strength)).astype(np.uint8)
        
        return denoised
    
    def record_reward(
        self,
        noise_params: dict,
        trm_accuracy: float
    ):
        """
        Record reward for RL training (SEAL loop).
        
        Args:
            noise_params: Noise parameters used
            trm_accuracy: Downstream TRM accuracy (reward)
        """
        self.strategy_rewards.append((noise_params, trm_accuracy))
    
    def update_with_rewards(self, optimizer: torch.optim.Optimizer):
        """
        Update adaptive generator using collected rewards (ReST-EM).
        
        Uses top-k rewards to reinforce good strategies.
        """
        if len(self.strategy_rewards) < 10:
            return
        
        # Sort by reward
        sorted_rewards = sorted(self.strategy_rewards, key=lambda x: x[1], reverse=True)
        
        # Take top 50%
        top_k = len(sorted_rewards) // 2
        best_strategies = sorted_rewards[:top_k]
        
        # Supervised learning on best strategies
        self.noise_generator.train()
        total_loss = 0
        
        for params, reward in best_strategies:
            # This would need text features - simplified for now
            # In practice, store (text_features, params, reward) tuples
            pass
        
        # Clear buffer
        self.strategy_rewards = []
    
    def save_adaptive_generator(self, path: str):
        """Save adaptive noise generator."""
        torch.save({
            'model_state_dict': self.noise_generator.state_dict(),
        }, path)
        print(f"âœ“ Adaptive generator saved to {path}")


class N2NFeatureAdapter(nn.Module):
    """
    Noise2Noise-inspired feature adapter for pretrained vision encoders.
    
    Takes pretrained features (CLIP/DINOv2/SigLIP) and:
    1. Denoises artifacts from pretraining
    2. Aligns text vs image modalities
    3. Adapts features for reasoning tasks (ARC-AGI)
    
    Training: Unsupervised N2N on augmented views
    - feat1 = encoder(aug1(image))
    - feat2 = encoder(aug2(image))
    - Loss: adapter(feat1) â†’ feat2 (and vice versa)
    
    Benefits:
    - No labels needed
    - Learns consensus representation
    - Removes pretrained biases
    - Task-specific adaptation
    """
    
    def __init__(
        self,
        input_dim: int = 768,
        hidden_dim: int = 768,
        num_layers: int = 3,
        dropout: float = 0.1
    ):
        """
        Args:
            input_dim: Pretrained feature dimension (768 for CLIP/DINOv2)
            hidden_dim: Hidden layer dimension
            num_layers: Number of transformer layers for adaptation
            dropout: Dropout rate
        """
        super().__init__()
        
        self.input_dim = input_dim
        
        # Lightweight transformer for feature refinement
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=input_dim,
            nhead=8,
            dim_feedforward=hidden_dim * 4,
            dropout=dropout,
            activation='gelu',
            batch_first=True,
            norm_first=True  # Pre-norm for stability
        )
        self.adapter = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # Optional: learnable denoising strength per patch
        self.strength_head = nn.Sequential(
            BitLinear(input_dim, 1),
            nn.Sigmoid()
        )
        
    def forward(
        self,
        features: torch.Tensor,
        return_strength: bool = False
    ) -> torch.Tensor:
        """
        Denoise and adapt pretrained features.
        
        Args:
            features: [B, N, D] pretrained patch features
            return_strength: Whether to return denoising strength map
        
        Returns:
            clean_features: [B, N, D] adapted features
            (optional) strength: [B, N, 1] denoising strength per patch
        """
        # Refine features through transformer
        refined = self.adapter(features)
        
        # Compute adaptive denoising strength
        strength = self.strength_head(features)  # [B, N, 1]
        
        # Blend original and refined based on strength
        # High strength â†’ more denoising, Low strength â†’ keep original
        clean_features = refined * strength + features * (1 - strength)
        
        if return_strength:
            return clean_features, strength
        return clean_features
    
    def n2n_loss(
        self,
        feat1: torch.Tensor,
        feat2: torch.Tensor
    ) -> torch.Tensor:
        """
        Noise2Noise training loss.
        
        Args:
            feat1: [B, N, D] features from augmented view 1
            feat2: [B, N, D] features from augmented view 2
        
        Returns:
            Bidirectional N2N loss
        """
        # Predict feat2 from feat1
        pred2 = self.forward(feat1)
        loss_forward = F.mse_loss(pred2, feat2)
        
        # Predict feat1 from feat2
        pred1 = self.forward(feat2)
        loss_backward = F.mse_loss(pred1, feat1)
        
        # Symmetric loss
        return (loss_forward + loss_backward) / 2


class PretrainedVisionBackbone(nn.Module):
    """
    Wrapper for pretrained vision encoders (CLIP, DINOv2, SigLIP).
    
    Provides unified interface for different pretrained models.
    Features are frozen to prevent catastrophic forgetting.
    """
    
    def __init__(
        self,
        model_name: str = 'clip',  # 'clip', 'dinov2', 'siglip'
        freeze: bool = True
    ):
        super().__init__()
        
        self.model_name = model_name
        
        device_map = "cpu" if not torch.cuda.is_available() else None
        
        if model_name == 'clip':
            from transformers import CLIPVisionModel
            self.encoder = CLIPVisionModel.from_pretrained('openai/clip-vit-base-patch16', device_map=device_map)
            self.feature_dim = 768
        elif model_name == 'dinov2':
            # DINOv2 has better features for dense tasks
            self.encoder = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')
            self.feature_dim = 768
        elif model_name == 'siglip':
            from transformers import SiglipVisionModel
            self.encoder = SiglipVisionModel.from_pretrained('google/siglip-base-patch16-224', device_map=device_map)
            self.feature_dim = 768
        else:
            raise ValueError(f"Unknown model: {model_name}")
        
        if freeze:
            for param in self.encoder.parameters():
                param.requires_grad = False
            self.encoder.eval()
        
        print(f"Loaded {model_name} (frozen={freeze}, dim={self.feature_dim})")
    
    def forward(self, images: torch.Tensor) -> torch.Tensor:
        """
        Extract pretrained features.
        
        Args:
            images: [B, 3, 224, 224] input images
        
        Returns:
            features: [B, N, D] patch features (N=196 for 16x16 patches)
        """
        with torch.no_grad() if not self.training else torch.enable_grad():
            if self.model_name == 'clip':
                outputs = self.encoder(images)
                features = outputs.last_hidden_state  # [B, 197, 768] (includes CLS)
                features = features[:, 1:, :]  # Remove CLS token â†’ [B, 196, 768]
            elif self.model_name == 'dinov2':
                features = self.encoder.forward_features(images)['x_norm_patchtokens']
            elif self.model_name == 'siglip':
                outputs = self.encoder(images)
                features = outputs.last_hidden_state[:, 1:, :]  # Remove CLS
        
        return features


if __name__ == "__main__":
    # Quick test
    print("ğŸ§ª Testing Noise2Noise Denoiser...")
    
    # Create dummy noisy images
    noisy1 = torch.rand(2, 3, 224, 224)  # Batch of 2
    noisy2 = torch.rand(2, 3, 224, 224)
    
    # Initialize denoiser
    denoiser = Noise2NoiseDenoiser()
    
    # Test inference
    img_np = (noisy1[0].permute(1, 2, 0).numpy() * 255).astype(np.uint8)
    denoised = denoiser.denoise(img_np)
    
    print(f"âœ“ Input shape: {img_np.shape}")
    print(f"âœ“ Output shape: {denoised.shape}")
    print(f"âœ“ Model parameters: {sum(p.numel() for p in denoiser.model.parameters()):,}")
    
    # Test SEAL components
    print("\nğŸ¦­ Testing SEAL-enhanced denoiser...")
    seal_denoiser = SEALNoise2Noise()
    text_features = torch.randn(1, 768)  # Dummy text embedding
    adaptive_denoised = seal_denoiser.denoise_adaptive(img_np, text_features)
    print(f"âœ“ Adaptive denoising works")
    print(f"âœ“ Adaptive generator parameters: {sum(p.numel() for p in seal_denoiser.noise_generator.parameters()):,}")
    
    # Test N2N Feature Adapter
    print("\nğŸ”§ Testing N2N Feature Adapter...")
    adapter = N2NFeatureAdapter(input_dim=768)
    feat1 = torch.randn(2, 196, 768)  # Pretrained features
    feat2 = torch.randn(2, 196, 768)
    clean = adapter(feat1)
    loss = adapter.n2n_loss(feat1, feat2)
    print(f"âœ“ Adapter output: {clean.shape}")
    print(f"âœ“ N2N loss: {loss.item():.4f}")
    print(f"âœ“ Adapter parameters: {sum(p.numel() for p in adapter.parameters()):,}")
    
    # Test Pretrained Backbone
    print("\nğŸŒ Testing Pretrained Vision Backbone...")
    try:
        backbone = PretrainedVisionBackbone(model_name='clip')
        images = torch.randn(2, 3, 224, 224)
        features = backbone(images)
        print(f"âœ“ CLIP features: {features.shape}")
    except Exception as e:
        print(f"CLIP test skipped: {e}")
    
    print("\nAll tests passed!")
