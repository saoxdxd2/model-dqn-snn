dab2e42da43bc44934cb07d7973984cad6a4407a


import shutil
from pathlib import Path
import os

# Zip the local cache
local_cache = Path("datasets/vision_unified/text_cache")
drive_base = Path("/content/drive/MyDrive/model_checkpoints")

if local_cache.exists():
    print("üì¶ Zipping cache...")
    
    # Create Drive directory
    drive_base.mkdir(parents=True, exist_ok=True)
    
    # Zip the cache (much faster than copying individual files)
    zip_file = drive_base / "text_cache_backup"
    shutil.make_archive(str(zip_file), 'zip', str(local_cache))
    
    zip_size = os.path.getsize(str(zip_file) + ".zip") / (1024**3)
    print(f"‚úÖ Zipped to Drive: text_cache_backup.zip ({zip_size:.2f}GB)")
    print(f"   Location: {zip_file}.zip")
    print(f"   Will sync to E:\\ automatically")
else:
    print("‚ö†Ô∏è No local cache found")



# === Complete Colab Setup - Syncs with E:\model_checkpoints ===
import os, shutil
from pathlib import Path

# 1. Mount Drive (configured to sync to E:\model_checkpoints on your PC)
try:
    from google.colab import drive
    drive.mount('/content/drive', force_remount=False)
    os.makedirs('/content/drive/MyDrive/model_checkpoints', exist_ok=True)
    print("‚úÖ Drive mounted (syncs to E:\\model_checkpoints)")
except: 
    print("‚ö†Ô∏è  Drive mount failed")

# 2. Update repo
%cd /content
if os.path.exists('model-dqn-snn'):
    %cd model-dqn-snn
    !git reset --hard HEAD
    !git pull origin main
    print("‚úÖ Repo updated")
else:
    !git clone https://github.com/saoxdxd2/model-dqn-snn.git
    %cd model-dqn-snn
    print("‚úÖ Repo cloned")

# 3. Restore data from Drive (syncs from E:\model_checkpoints)
print("üì¶ Checking Drive for cached data (synced from E:\\model_checkpoints)...")

# Text cache (rendered images)
drive_cache = Path('/content/drive/MyDrive/model_checkpoints/text_cache')
local_cache = Path('datasets/vision_unified/text_cache')

if drive_cache.exists():
    cache_files = list(drive_cache.glob('*.png'))
    print(f"   Found text cache: {len(cache_files)} images")
    print("   Restoring to Colab...")
    local_cache.parent.mkdir(parents=True, exist_ok=True)
    shutil.copytree(drive_cache, local_cache, dirs_exist_ok=True)
    print("‚úÖ Cache restored (encoding will be 10x faster)")
else:
    print("‚ÑπÔ∏è  No cache yet (first run will populate and sync to E:\\)")

# Check for encoding chunks
drive_chunks = Path('/content/drive/MyDrive/model_checkpoints/encoding_progress')
if drive_chunks.exists():
    chunks = list(drive_chunks.glob('consolidated_*.pt'))
    if chunks:
        print(f"üìö Found {len(chunks)} encoding chunks (synced from E:\\)")

# 4. Install dependencies
!pip install -q --no-cache-dir -r requirements.txt

# 5. Start training
print("\n" + "="*70)
print("üöÄ Starting training with Drive‚ÜîE:\\ sync")
print("   First run: Populate cache ‚Üí Sync to E:\\model_checkpoints")
print("   Later runs: Load from E:\\ ‚Üí 10x faster")
print("   All data auto-syncs: Colab ‚Üî Drive ‚Üî E:\\")
print("="*70 + "\n")
!python train.py