dab2e42da43bc44934cb07d7973984cad6a4407a

# === Colab Setup with Local Tar Transfer (No Drive) ===
import os, shutil, tarfile
from pathlib import Path
from google.colab import files

print("="*70)
print("üõ†Ô∏è  Colab Setup - Local Transfer Mode")
print("   No Google Drive needed")
print("   Transfer via tar upload/download (faster, no compression)")
print("="*70 + "\n")

# 1. Clone or update repo
%cd /content
if os.path.exists('model-dqn-snn'):
    %cd model-dqn-snn
    !git reset --hard HEAD
    !git pull origin main
    print("‚úÖ Repo updated")
else:
    !git clone https://github.com/saoxdxd2/model-dqn-snn.git
    %cd model-dqn-snn
    print("‚úÖ Repo cloned")

# 2. Check for existing data or upload from local machine
checkpoints_dir = Path('model_checkpoints')
dataset_chunks_dir = Path('datasets/vision_unified/stream_checkpoints')

# Check what we already have
has_model_ckpt = checkpoints_dir.exists()
has_dataset_chunks = dataset_chunks_dir.exists() and list(dataset_chunks_dir.glob('consolidated_*.pt'))

if not has_model_ckpt and not has_dataset_chunks:
    print("\nüì§ No checkpoints or dataset chunks found. Upload tar from your E:\\ drive?")
    print("   Expected: model-dqn-snn.tar (contains model_checkpoints/ and/or datasets/)")
    response = input("Upload tar? (y/n): ").lower()
    
    if response == 'y':
        print("üì¶ Upload your model-dqn-snn.tar file...")
        uploaded = files.upload()
        
        if uploaded:
            tar_filename = list(uploaded.keys())[0]
            print(f"\nüìú Extracting {tar_filename}...")
            
            with tarfile.open(tar_filename, 'r') as tar:
                tar.extractall('/content/temp_extract')
            
            temp_base = Path('/content/temp_extract/model-dqn-snn')
            
            # Restore model checkpoints
            extracted_checkpoints = temp_base / 'model_checkpoints'
            if extracted_checkpoints.exists():
                shutil.copytree(extracted_checkpoints, checkpoints_dir, dirs_exist_ok=True)
                print("‚úÖ Restored model checkpoints")
            
            # Restore dataset consolidated chunks (pre-encoded capsules)
            extracted_dataset = temp_base / 'datasets'
            if extracted_dataset.exists():
                Path('datasets').mkdir(exist_ok=True)
                shutil.copytree(extracted_dataset, 'datasets', dirs_exist_ok=True)
                
                # Count consolidated chunks
                chunks = list(dataset_chunks_dir.glob('consolidated_*.pt')) if dataset_chunks_dir.exists() else []
                if chunks:
                    total_gb = sum(c.stat().st_size for c in chunks) / (1024**3)
                    print(f"‚úÖ Restored {len(chunks)} pre-encoded chunk(s) ({total_gb:.2f}GB)")
                    print(f"   üöÄ Training can start immediately on existing data!")
            
            # Cleanup
            shutil.rmtree('/content/temp_extract')
            os.remove(tar_filename)
        else:
            print("‚ÑπÔ∏è  No file uploaded, starting fresh")
    else:
        print("‚ÑπÔ∏è  Skipping upload, starting fresh")
else:
    if has_model_ckpt:
        print("‚úÖ Found existing model_checkpoints/")
    if has_dataset_chunks:
        chunks = list(dataset_chunks_dir.glob('consolidated_*.pt'))
        total_gb = sum(c.stat().st_size for c in chunks) / (1024**3)
        print(f"‚úÖ Found {len(chunks)} pre-encoded chunk(s) ({total_gb:.2f}GB)")
        print(f"   üöÄ Training will start on existing data!")

# 3. Install dependencies
print("\nüì¶ Installing dependencies from requirements.txt...")
print("   This may take 2-3 minutes on first run\n")
%cd /content/model-dqn-snn
!pip install --no-cache-dir -r requirements.txt
print("\n‚úÖ Dependencies installed")

# 4. Start training
print("\n" + "="*70)
print("üöÄ Starting Training - Hybrid Pretrained Pipeline")
print("   Config: hybrid_pretrained.yaml (CLIP+ViT+N2N+TRM+COCONUT)")
print("   Optimizations: fp16 + torch.compile + batch=192")
print("   Time: ~1h (100k samples) or ~10h (1M samples)")
print("   Resume: Auto-continues from model_checkpoints/")
print("="*70 + "\n")
!python train.py




!find datasets/vision_unified/text_cache -name "*.npy" -type f -delete
import tarfile
import os
import shutil

os.chdir('/content')

print('üì¶ Creating tar with recursive space-saving mode...')
print('   Archives deepest subdirectories first, deletes as it goes\n')

def get_all_subdirs(base_path):
    """Get all subdirectories sorted by depth (deepest first)."""
    all_dirs = []
    for root, dirs, files in os.walk(base_path):
        for d in dirs:
            full_path = os.path.join(root, d)
            depth = full_path.count(os.sep)
            all_dirs.append((depth, full_path))
    # Sort by depth (deepest first) so we delete leaf directories first
    all_dirs.sort(reverse=True, key=lambda x: x[0])
    return [path for _, path in all_dirs]

def get_dir_size(path):
    """Get total size of directory in MB."""
    total = 0
    for root, dirs, files in os.walk(path):
        for f in files:
            fp = os.path.join(root, f)
            if os.path.exists(fp):
                total += os.path.getsize(fp)
    return total / (1024**2)

base_dir = 'model-dqn-snn'

# Get all subdirectories (deepest first)
all_subdirs = get_all_subdirs(base_dir)
print(f'Found {len(all_subdirs)} subdirectories to process\n')

with tarfile.open('model-dqn-snn.tar', 'w') as tar:
    # Archive deepest directories first
    for i, dir_path in enumerate(all_subdirs, 1):
        if not os.path.exists(dir_path):  # Skip if already deleted
            continue
            
        rel_path = os.path.relpath(dir_path, '.')
        dir_size = get_dir_size(dir_path)
        
        print(f'{i}/{len(all_subdirs)} {rel_path} (~{dir_size:.0f}MB)')
        tar.add(dir_path, arcname=rel_path)
        shutil.rmtree(dir_path)
        print(f'         ‚úÖ Freed ~{dir_size:.0f}MB\n')
    
    # Archive remaining files in base directory
    print('Adding remaining files...')
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isfile(item_path):
            tar.add(item_path, arcname=item_path)
    print('    ‚úÖ Done\n')

print('‚úÖ Successfully created model-dqn-snn.tar')
print('   All source directories deleted to save space')
print('   Downloading...\n')

from google.colab import files
files.download('model-dqn-snn.tar')