dab2e42da43bc44934cb07d7973984cad6a4407a

# === Colab Setup with Local Tar Transfer (No Drive) ===
import os, shutil, tarfile
from pathlib import Path
from google.colab import files

print("="*70)
print("üõ†Ô∏è  Colab Setup - Local Transfer Mode")
print("   No Google Drive needed")
print("   Transfer via tar upload/download (faster, no compression)")
print("="*70 + "\n")

# 1. Clone or update repo
%cd /content
if os.path.exists('model-dqn-snn'):
    %cd model-dqn-snn
    !git reset --hard HEAD
    !git pull origin main
    print("‚úÖ Repo updated")
else:
    !git clone https://github.com/saoxdxd2/model-dqn-snn.git
    %cd model-dqn-snn
    print("‚úÖ Repo cloned")

# 2. Check for existing model_checkpoints/ or upload from local machine
checkpoints_dir = Path('model_checkpoints')

if not checkpoints_dir.exists():
    print("\nüì§ No checkpoints found. Upload tar from your E:\\ drive?")
    print("   Expected: model-dqn-snn.tar (contains model_checkpoints/)")
    response = input("Upload tar? (y/n): ").lower()
    
    if response == 'y':
        print("üì¶ Upload your model-dqn-snn.tar file...")
        uploaded = files.upload()
        
        if uploaded:
            tar_filename = list(uploaded.keys())[0]
            print(f"\nüìú Extracting {tar_filename}...")
            
            with tarfile.open(tar_filename, 'r') as tar:
                tar.extractall('/content/temp_extract')
            
            # Find and copy model_checkpoints from extracted content
            extracted_checkpoints = Path('/content/temp_extract/model-dqn-snn/model_checkpoints')
            if extracted_checkpoints.exists():
                shutil.copytree(extracted_checkpoints, checkpoints_dir, dirs_exist_ok=True)
                
                # Count restored files
                cache_files = len(list((checkpoints_dir / 'text_cache').glob('*.png'))) if (checkpoints_dir / 'text_cache').exists() else 0
                chunks = len(list((checkpoints_dir / 'encoding_progress').glob('*.pt'))) if (checkpoints_dir / 'encoding_progress').exists() else 0
                
                print(f"‚úÖ Restored from local machine:")
                print(f"   Text cache: {cache_files} images")
                print(f"   Encoding chunks: {chunks} files")
                
                # Cleanup
                shutil.rmtree('/content/temp_extract')
                os.remove(tar_filename)
            else:
                print("‚ö†Ô∏è  No model_checkpoints/ found in tar")
        else:
            print("‚ÑπÔ∏è  No file uploaded, starting fresh")
    else:
        print("‚ÑπÔ∏è  Skipping upload, starting fresh")
else:
    print("‚úÖ Found existing model_checkpoints/ (from previous session)")

# 3. Install dependencies
print("\nüì¶ Installing dependencies from requirements.txt...")
print("   This may take 2-3 minutes on first run\n")
%cd /content/model-dqn-snn
!pip install --no-cache-dir -r requirements.txt
print("\n‚úÖ Dependencies installed")

# 4. Start training
print("\n" + "="*70)
print("üöÄ Starting Training")
print("   First run: ~5.6 hours (cache + encode)")
print("   Resume: Continues from model_checkpoints/")
print("   After: Run tar cell to download back to E:\\")
print("="*70 + "\n")
!python train.py




!find datasets/vision_unified/text_cache -name "*.npy" -type f -delete
import tarfile
import os
import shutil

os.chdir('/content')

print('üì¶ Creating tar with recursive space-saving mode...')
print('   Archives deepest subdirectories first, deletes as it goes\n')

def get_all_subdirs(base_path):
    """Get all subdirectories sorted by depth (deepest first)."""
    all_dirs = []
    for root, dirs, files in os.walk(base_path):
        for d in dirs:
            full_path = os.path.join(root, d)
            depth = full_path.count(os.sep)
            all_dirs.append((depth, full_path))
    # Sort by depth (deepest first) so we delete leaf directories first
    all_dirs.sort(reverse=True, key=lambda x: x[0])
    return [path for _, path in all_dirs]

def get_dir_size(path):
    """Get total size of directory in MB."""
    total = 0
    for root, dirs, files in os.walk(path):
        for f in files:
            fp = os.path.join(root, f)
            if os.path.exists(fp):
                total += os.path.getsize(fp)
    return total / (1024**2)

base_dir = 'model-dqn-snn'

# Get all subdirectories (deepest first)
all_subdirs = get_all_subdirs(base_dir)
print(f'Found {len(all_subdirs)} subdirectories to process\n')

with tarfile.open('model-dqn-snn.tar', 'w') as tar:
    # Archive deepest directories first
    for i, dir_path in enumerate(all_subdirs, 1):
        if not os.path.exists(dir_path):  # Skip if already deleted
            continue
            
        rel_path = os.path.relpath(dir_path, '.')
        dir_size = get_dir_size(dir_path)
        
        print(f'{i}/{len(all_subdirs)} {rel_path} (~{dir_size:.0f}MB)')
        tar.add(dir_path, arcname=rel_path)
        shutil.rmtree(dir_path)
        print(f'         ‚úÖ Freed ~{dir_size:.0f}MB\n')
    
    # Archive remaining files in base directory
    print('Adding remaining files...')
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isfile(item_path):
            tar.add(item_path, arcname=item_path)
    print('    ‚úÖ Done\n')

print('‚úÖ Successfully created model-dqn-snn.tar')
print('   All source directories deleted to save space')
print('   Downloading...\n')

from google.colab import files
files.download('model-dqn-snn.tar')