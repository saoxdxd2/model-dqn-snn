# Vision Encoder Configurations for Multimodal Generalist Models

# CLIP Vision Encoder (Best for multimodal alignment)
clip_config:
  encoder_type: 'clip'
  clip_model: 'openai/clip-vit-base-patch16'  # 86M params, 768 dim
  # Alternatives:
  # - 'openai/clip-vit-large-patch14': 304M params, 1024 dim (higher quality)
  # - 'laion/CLIP-ViT-H-14-laion2B-s32B-b79K': 632M params, 1024 dim (best)
  hidden_size: 768
  freeze_encoder: true
  use_hybrid: false

# DINOv2 (Best for complex visual reasoning)
dinov2_config:
  encoder_type: 'dinov2'
  dinov2_model: 'facebook/dinov2-base'  # 86M params, 768 dim
  # Alternatives:
  # - 'facebook/dinov2-large': 304M params, 1024 dim
  # - 'facebook/dinov2-giant': 1.1B params, 1536 dim (best for complex tasks)
  hidden_size: 768
  freeze_encoder: true
  use_hybrid: false

# Hybrid (CLIP + DINOv2 for maximum capability)
hybrid_config:
  encoder_type: 'hybrid'
  clip_model: 'openai/clip-vit-base-patch16'
  dinov2_model: 'facebook/dinov2-base'
  hidden_size: 768  # Will project from 1536 (768+768) to 768
  freeze_encoder: true
  use_hybrid: true

# Legacy CNN (for backward compatibility only)
cnn_legacy_config:
  encoder_type: 'cnn'
  use_legacy_cnn: true
  hidden_size: 256
  num_conv_layers: 3
  conv_channels: [64, 128, 256]

# Recommendations based on use case:
#
# 1. General multimodal (text + vision):
#    Use CLIP - maintains alignment between modalities
#
# 2. Complex visual reasoning (ARC, puzzles):
#    Use DINOv2 - best spatial understanding
#
# 3. Maximum capability (if you have compute):
#    Use Hybrid - combines multimodal + complex reasoning
#
# 4. Fast inference / low memory:
#    Use CLIP-base (smallest pretrained option)
