# Text Generation Training Config
# Small-scale TRM for language modeling on local hardware

defaults:
  - arch: trm_text
  - _self_

hydra:
  output_subdir: null

# Data path
data_paths: ['data/text-wikitext2']
data_paths_test: []

evaluators: []  # Can add perplexity evaluator later

# Hyperparams - Training (optimized for 15GB T4 GPU)
# With gradient checkpointing: +15% memory → +20% batch size possible
global_batch_size: 154  # Increased from 128 (20% boost from checkpointing)
epochs: 5000  # Extended for better convergence on small dataset
eval_interval: 5  # Save every 5 epochs (~366 steps)
checkpoint_every_eval: True

# Learning rates
lr: 3e-4  # Standard for small LMs
lr_min_ratio: 0.1
lr_warmup_steps: 500

# Optimizer settings
beta1: 0.9
beta2: 0.95
weight_decay: 0.01  # Light regularization

# No puzzle embeddings for text
puzzle_emb_lr: 0.0
puzzle_emb_weight_decay: 0.0

seed: 42
min_eval_interval: 0

checkpoint_path: "checkpoints/text-trm-10m"

ema: True  # Exponential Moving Average for stability
ema_rate: 0.9995  # Slightly higher for text (smoother averaging)
freeze_weights: False

# Automatic Mixed Precision (T4 GPU optimization)
use_amp: True  # 2-3× faster training on T4 Tensor Cores
amp_dtype: "float16"  # float16 for T4, bfloat16 for A100+

# Optuna Hyperparameter Optimization (runs BEFORE main training)
enable_optuna_sweep: False  # Set True to run hyperparameter search first
optuna_n_trials: 20  # Number of trials to run (20 trials ≈ 3-4 hours)
optuna_timeout: 14400  # Timeout in seconds (14400 = 4 hours)
optuna_study_name: "trm-text-optimization"
