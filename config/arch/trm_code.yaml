name: recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1
loss:
  name: losses@ACTLossHead
  loss_type: softmax_cross_entropy
  enable_dqn: False
  deep_supervision_steps: 4

# Dataset parameters - CodeGen tokenizer
seq_len: 1023  # 1024 - 1 for next-token (longer for code)
vocab_size: 51200  # CodeGen vocab
batch_size: 12  # Reduced for larger model + MTP overhead

# Halting parameters
halt_exploration_prob: 0.15  # Higher exploration for code diversity
halt_max_steps: 16  # More cycles for complex code reasoning

# Recursion Depth (optimized for code reasoning)
H_cycles: 4  # Increased: code needs more reasoning steps
L_cycles: 3  # Increased: deeper local refinement

H_layers: 0
L_layers: 3  # Increased to 3 for better code understanding

# Model size
hidden_size: 384  # Increased from 256 for better code representations
num_heads: 6  # Proportional to hidden_size
num_key_value_heads: 2  # GQA: 3:1 ratio for efficiency
expansion: 3  # Increased FFN capacity for complex code patterns

# No puzzle embeddings
puzzle_emb_ndim: 0
num_puzzle_identifiers: 1

# Code-specific settings
pos_encodings: rope
rope_theta: 10000
rms_norm_eps: 1e-6
forward_dtype: float16
causal: True  # CRITICAL for code generation

# Optimizations
mlp_t: False
puzzle_emb_len: 0
no_ACT_continue: True
use_learned_halting_eval: True

# Deep Supervision (critical for code correctness)
deep_supervision_steps: 8  # More supervision for code
enable_gradient_checkpointing: True

# EMA for stability
use_ema: True  # Enable for code stability
ema_decay: 0.9995  # Slower decay for code

# DQN enabled for adaptive compute on complex code
enable_dqn: True
dqn_buffer_capacity: 15000  # Larger buffer for code diversity
dqn_buffer_min_size: 2000
dqn_batch_size: 256
dqn_gamma: 0.99  # Long-horizon planning for code
dqn_target_tau: 0.005
dqn_epsilon_start: 0.4
dqn_epsilon_end: 0.05
dqn_epsilon_decay_steps: 100000

# Selective storage for DQN (20-40% memory savings)
dqn_td_threshold: 0.015  # Slightly higher for code complexity

# Reward shaping for code generation
reward_step_penalty: 0.015  # Lower penalty (code needs more steps)
reward_terminal_correct: 1.0
reward_terminal_incorrect: -0.3  # Less harsh for partial code

# Q-Head optimized for code
q_head_type: rnn  # RNN captures code structure better
q_head_hidden_size: 192
q_head_num_layers: 2

# Memory bank for code patterns (CRITICAL for code reuse)
enable_memory: True
memory_capacity: 8192  # Large capacity for diverse patterns
memory_num_heads: 6
memory_dropout: 0.1
memory_reward_bonus: 0.3  # Reward pattern reuse
memory_reward_threshold: 0.1

# Multi-Token Prediction (META research: 12-17% boost on code!)
enable_mtp: True
mtp_num_depths: 4  # Predict 4 future tokens
mtp_loss_weight: 0.6  # Higher weight for code (structured output)
mtp_share_embeddings: True
mtp_share_output_head: False  # Independent heads for better accuracy

# CPU-friendly
cpu_optimized: False
use_gelu: True
ffn_expansion_ratio: 2.0
