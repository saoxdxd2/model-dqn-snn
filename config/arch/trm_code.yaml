name: recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1
loss:
  name: losses@ACTLossHead
  loss_type: softmax_cross_entropy
  enable_dqn: False
  deep_supervision_steps: 4

# Dataset parameters - CodeGen tokenizer
seq_len: 1023  # 1024 - 1 for next-token (longer for code)
vocab_size: 51200  # CodeGen vocab
batch_size: 16  # Smaller batch for longer sequences

# Halting parameters
halt_exploration_prob: 0.1
halt_max_steps: 12  # More cycles for code reasoning

# Recursion Depth
H_cycles: 3  # More reasoning for code
L_cycles: 2  

H_layers: 0
L_layers: 2

# Model size
hidden_size: 256
num_heads: 4
num_key_value_heads: 4
expansion: 2

# No puzzle embeddings
puzzle_emb_ndim: 0
num_puzzle_identifiers: 1

# Code-specific settings
pos_encodings: rope
rope_theta: 10000
rms_norm_eps: 1e-6
forward_dtype: bfloat16
causal: True  # CRITICAL for code generation

# Optimizations
mlp_t: False
puzzle_emb_len: 0
no_ACT_continue: True
use_learned_halting_eval: True

# Deep Supervision
deep_supervision_steps: 4
enable_gradient_checkpointing: True

# EMA
use_ema: False
ema_decay: 0.999

# DQN disabled
enable_dqn: False

# Memory disabled initially
enable_memory: False

# MTP disabled
enable_mtp: False

# CPU-friendly
cpu_optimized: False
use_gelu: True
ffn_expansion_ratio: 2.0
