name: recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1
loss:
  name: losses@ACTLossHead
  loss_type: softmax_cross_entropy
  enable_dqn: True  # Enable DQN for adaptive reasoning
  deep_supervision_steps: 6

# Dataset parameters
seq_len: 64  # 8x8 spatial grid from CNN stem (was 16 for patch tokens)
vocab_size: 10  # CIFAR-10 classes (output)
batch_size: 32

# CNN Tokenizer (CCT-style convolutional stem)
use_cnn_tokenizer: True  # Use CNN instead of K-Means patch embedding
cnn_in_channels: 3  # RGB images
cnn_conv_channels: [64, 128, 256]  # Progressive feature extraction

# Halting parameters
halt_exploration_prob: 0.05
halt_max_steps: 10  # Vision tasks converge faster with CNN features

# Recursion Depth (balanced for vision)
H_cycles: 3
L_cycles: 2

H_layers: 0
L_layers: 2

# Model size (optimized for patch tokens)
hidden_size: 256  # Medium size (each patch token is rich)
num_heads: 8
num_key_value_heads: 8
expansion: 4  # Standard expansion

# No puzzle embeddings (patches are the tokens)
puzzle_emb_ndim: 0
num_puzzle_identifiers: 1

# Vision-specific settings
pos_encodings: rope  # RoPE works well for spatial relationships
rope_theta: 10000
rms_norm_eps: 1e-6
forward_dtype: float16
causal: False  # CRITICAL: Bidirectional for image understanding

# Optimizations
mlp_t: False
puzzle_emb_len: 0  # CRITICAL: Must be 0 when puzzle_emb_ndim=0
no_ACT_continue: True
use_learned_halting_eval: True

# Deep Supervision
deep_supervision_steps: 6
enable_gradient_checkpointing: True

# EMA for stability
use_ema: True
ema_decay: 0.999

# DQN enabled (learn when to stop reasoning)
enable_dqn: True
dqn_buffer_capacity: 10000
dqn_buffer_min_size: 1000
dqn_batch_size: 128
dqn_gamma: 0.95
dqn_target_tau: 0.005
dqn_epsilon_start: 0.3
dqn_epsilon_end: 0.05
dqn_epsilon_decay_steps: 50000

# Selective storage (Schaul et al., 2015: 20-40% memory savings)
dqn_td_threshold: 0.01  # Only store transitions with |TD-error| > threshold (0.0 = disabled)

# Reward shaping for vision
reward_step_penalty: 0.02
reward_terminal_correct: 1.0
reward_terminal_incorrect: -0.5

# Q-Head type
q_head_type: mini_attention  # Context-aware halting
q_head_hidden_size: 128
q_head_num_layers: 4

# Memory bank (store visual patterns)
enable_memory: True
memory_capacity: 2048
memory_num_heads: 8  # Must divide hidden_size (256)
memory_dropout: 0.1
memory_reward_bonus: 0.3
memory_reward_threshold: 0.6

# Intrinsic curiosity
enable_count_curiosity: True
enable_rnd_curiosity: True
curiosity_count_beta: 0.05
curiosity_rnd_weight: 0.1

# Entropy regularization
enable_entropy_regularization: True
entropy_regularization_weight: 0.01

# Prioritized replay
enable_prioritized_replay: True
per_alpha: 0.6
per_beta: 0.4

# MTP disabled for vision
enable_mtp: False

# CPU settings
cpu_optimized: False
use_gelu: True
ffn_expansion_ratio: 4.0
