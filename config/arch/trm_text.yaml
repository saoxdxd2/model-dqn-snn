name: recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1
loss:
  name: losses@ACTLossHead
  loss_type: softmax_cross_entropy  # Standard for text
  enable_dqn: False  # Keep simple for initial training
  deep_supervision_steps: 4  # Moderate supervision

# Dataset parameters (CRITICAL - must match tokenizer)
seq_len: 511  # 512 - 1 for next-token prediction
vocab_size: 50257  # GPT-2 BPE vocab
batch_size: 32  # Match global_batch_size in cfg_text.yaml

# Halting parameters
halt_exploration_prob: 0.1
halt_max_steps: 8  # Reduced for text (faster convergence)

# Recursion Depth (optimized for text)
H_cycles: 2  # Lighter recursion for text
L_cycles: 2  

H_layers: 0
L_layers: 2

# Model size (small for local training)
hidden_size: 256  # Reduced from 512 for ~13M params
num_heads: 4
num_key_value_heads: 4
expansion: 2  # Reduced for efficiency

# No puzzle embeddings for text (CRITICAL)
puzzle_emb_ndim: 0
num_puzzle_identifiers: 1  # Just blank ID

# Text-specific settings
pos_encodings: rope
rope_theta: 10000  # Standard RoPE base
rms_norm_eps: 1e-6  # LayerNorm epsilon
forward_dtype: bfloat16
causal: True  # CRITICAL: Enable causal attention for autoregressive generation

# Optimizations
mlp_t: False
puzzle_emb_len: 0  # No puzzle embeddings
no_ACT_continue: True
use_learned_halting_eval: True

# Deep Supervision
deep_supervision_steps: 4
enable_gradient_checkpointing: True

# EMA for stability
use_ema: False  # Disable for faster initial training
ema_decay: 0.999

# DQN disabled for simplicity
enable_dqn: False

# Memory bank disabled initially
enable_memory: False

# Multi-Token Prediction disabled
enable_mtp: False

# CPU-friendly settings
cpu_optimized: False
use_gelu: True
ffn_expansion_ratio: 2.0
