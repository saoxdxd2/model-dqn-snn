name: recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1
loss:
  name: losses@ACTLossHead
  loss_type: softmax_cross_entropy  # Standard for text
  enable_dqn: False  # Keep simple for initial training
  deep_supervision_steps: 4  # Moderate supervision

# Dataset parameters (CRITICAL - HESC: Hierarchical Expandable Semantic Capsules (10-50x compression)
use_semantic_encoder: true
freeze_capsule_encoder: true  # Stage A: freeze encoder
target_capsules: 12  # k coarse capsules (concept-level)
children_per_capsule: 4  # m children tokens per capsule
checksum_dim: 32  # Reconstructability signature size
enable_capsule_expansion: true  # DQN can expand capsules on-demand

# Encoder Configuration (Input: text → capsules)
encoder_model: "openai/clip-vit-large-patch14"  # Options: CLIP, BERT, RoBERTa, etc.

# Output Configuration (Capsules → Concepts)
num_concepts: 2048  # Semantic concept vocabulary (vs 50k BPE tokens)
use_vq_codebook: true  # Vector quantization for discrete concepts
# Total output vocab = num_concepts + 4 control symbols (<EXPAND>, <STOP>, <MERGE>, <PAD>)

# Legacy fallback (for non-capsule datasets)
seq_len: 1024
vocab_size: 2052  # num_concepts + control symbols (overridden at runtime)
batch_size: 128

# Halting parameters
halt_exploration_prob: 0.1
halt_max_steps: 8  # Reduced for text (faster convergence)

# Recursion Depth (optimized for text)
H_cycles: 3  # Increased from 2 for deeper reasoning
L_cycles: 3  

H_layers: 0
L_layers: 3  # Increased from 2 for more capacity

# Model size (scaled for 15GB GPU)
hidden_size: 512  # Increased from 256 for ~60M params
num_heads: 8  # Increased proportionally
num_key_value_heads: 2  # GQA for efficiency (4:1 ratio)
expansion: 3  # Increased FFN capacity

# No puzzle embeddings for text (CRITICAL)
puzzle_emb_ndim: 0
num_puzzle_identifiers: 1  # Just blank ID

# Text-specific settings
pos_encodings: rope
rope_theta: 10000  # Standard RoPE base
rms_norm_eps: 1e-6  # LayerNorm epsilon
forward_dtype: float16
causal: True  # CRITICAL: Enable causal attention for autoregressive generation

# Optimizations
mlp_t: False
puzzle_emb_len: 0  # No puzzle embeddings
no_ACT_continue: True
use_learned_halting_eval: True

# Deep Supervision
deep_supervision_steps: 4
enable_gradient_checkpointing: True

# EMA for stability (improves final accuracy by 2-5%)
use_ema: True  # Maintains shadow copy of weights
ema_decay: 0.999  # Slow decay = stable shadow model

# DQN disabled for simplicity
enable_dqn: False

# Memory bank disabled initially
enable_memory: False

# Multi-Token Prediction (DeepSeek-V3: 2.5× better data efficiency)
enable_mtp: True
mtp_num_depths: 3  # Predict next 3 tokens simultaneously
mtp_loss_weight: 0.5  # Balance main loss vs MTP loss

# CPU-friendly settings
cpu_optimized: False
use_gelu: True
ffn_expansion_ratio: 2.0
