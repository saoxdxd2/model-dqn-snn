name: recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1
loss:
  name: losses@ACTLossHead
  loss_type: softmax_cross_entropy  # Standard for text
  enable_dqn: False  # Keep simple for initial training
  deep_supervision_steps: 4  # Moderate supervision

# Dataset parameters (CRITICAL - must match tokenizer)
seq_len: 1023  # Increased from 511 for better context (1024 - 1)
vocab_size: 50257  # GPT-2 BPE vocab
batch_size: 128  # Match global_batch_size in cfg_text.yaml

# Halting parameters
halt_exploration_prob: 0.1
halt_max_steps: 8  # Reduced for text (faster convergence)

# Recursion Depth (optimized for text)
H_cycles: 3  # Increased from 2 for deeper reasoning
L_cycles: 3  

H_layers: 0
L_layers: 3  # Increased from 2 for more capacity

# Model size (scaled for 15GB GPU)
hidden_size: 512  # Increased from 256 for ~60M params
num_heads: 8  # Increased proportionally
num_key_value_heads: 2  # GQA for efficiency (4:1 ratio)
expansion: 3  # Increased FFN capacity

# No puzzle embeddings for text (CRITICAL)
puzzle_emb_ndim: 0
num_puzzle_identifiers: 1  # Just blank ID

# Text-specific settings
pos_encodings: rope
rope_theta: 10000  # Standard RoPE base
rms_norm_eps: 1e-6  # LayerNorm epsilon
forward_dtype: float16
causal: True  # CRITICAL: Enable causal attention for autoregressive generation

# Optimizations
mlp_t: False
puzzle_emb_len: 0  # No puzzle embeddings
no_ACT_continue: True
use_learned_halting_eval: True

# Deep Supervision
deep_supervision_steps: 4
enable_gradient_checkpointing: True

# EMA for stability (improves final accuracy by 2-5%)
use_ema: True  # Maintains shadow copy of weights
ema_decay: 0.999  # Slow decay = stable shadow model

# DQN disabled for simplicity
enable_dqn: False

# Memory bank disabled initially
enable_memory: False

# Multi-Token Prediction (DeepSeek-V3: 2.5Ã— better data efficiency)
enable_mtp: True
mtp_num_depths: 3  # Predict next 3 tokens simultaneously
mtp_loss_weight: 0.5  # Balance main loss vs MTP loss

# CPU-friendly settings
cpu_optimized: False
use_gelu: True
ffn_expansion_ratio: 2.0
