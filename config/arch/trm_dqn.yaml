name: recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1
loss:
  name: losses@ACTLossHead
  loss_type: stablemax_cross_entropy
  enable_dqn: True  # Enable DQN enhancement
  deep_supervision_steps: 4  # TRM paper: multi-step supervision (+20% accuracy)

# ===== TRM Architecture (same as baseline) =====
halt_exploration_prob: 0.1  # Not used when DQN enabled
halt_max_steps: 16

# OPTIMAL (TRM paper): Deep recursion + small model
H_cycles: 16  # Paper: 16 H-cycles (from 3) → +7.9% accuracy
L_cycles: 1   # Paper: 1 L-cycle (from 6) → optimal

H_layers: 0
L_layers: 2   # Paper: 2 layers optimal (tested 2,4,8)

# CRITICAL: Smaller prevents overfitting on small datasets
hidden_size: 128  # Paper: 64-128 optimal (from 512)
num_heads: 4      # Scaled proportionally
expansion: 4

puzzle_emb_ndim: ${.hidden_size}

pos_encodings: rope
forward_dtype: float16

mlp_t: False
puzzle_emb_len: 16
no_ACT_continue: True

# ===== DQN Enhancement Parameters =====

# Disable DQN (paper: simple BCE > Q-learning for small datasets)
enable_dqn: False

# Replay Buffer
dqn_buffer_capacity: 20000
dqn_buffer_min_size: 5000
dqn_batch_size: 256

# Learning Parameters
dqn_gamma: 0.99  # Discount factor

# Target Network (Soft Polyak Update)
dqn_target_tau: 0.005  # Soft update rate (0.5% online, 99.5% target)

# Exploration (Epsilon-Greedy)
dqn_epsilon_start: 0.5   # Conservative start (not 1.0)
dqn_epsilon_end: 0.05
dqn_epsilon_decay_steps: 100000

# Reward Shaping (Simplified - 3 parameters)
reward_step_penalty: 0.01          # -0.01 per step (1 step = 1% accuracy trade-off)
reward_terminal_correct: 1.0       # +1.0 bonus for correct halt, scaled by efficiency
reward_terminal_incorrect: -0.5    # -0.5 penalty for incorrect early halt

# Q-Head Architecture (simplified)
q_head_type: "mlp"  # Simplified (RNN adds complexity for halting)
q_head_hidden_size: 64   # Scaled down with model
q_head_num_layers: 1

# Memory Bank Configuration (few-shot learning ready)
enable_memory: True       # Enable associative memory for pattern reuse
memory_capacity: 4096     # Number of memory slots
memory_num_heads: 4       # Scaled down with model
memory_dropout: 0.1       # Dropout for memory attention
memory_reward_bonus: 0.5  # Reward bonus for memory-assisted improvements
memory_reward_threshold: 0.05  # Minimum improvement (5%) to store in memory

# Export Configuration (for CPU inference)
export_to_snn: False     # Convert Q-head to Spiking Neural Network
export_to_bnn: False     # Convert Q-head to Binary Neural Network (1-bit weights)

# Notes:
# - Train with dense MLP/RNN Q-head on GPU for stability
# - Export to SNN/BNN for 10-100× energy efficiency on CPU/neuromorphic hardware
# - Simplified reward: r = Δacc - 0.01*step + terminal_bonus
# - Epsilon decay: 0.5 → 0.05 over 100K steps
# - To disable DQN, set enable_dqn: False
