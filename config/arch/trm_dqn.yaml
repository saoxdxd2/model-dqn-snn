name: recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1
loss:
  name: losses@ACTLossHead
  loss_type: stablemax_cross_entropy
  enable_dqn: True  # Enable DQN enhancement

# ===== TRM Architecture (same as baseline) =====
halt_exploration_prob: 0.1  # Not used when DQN enabled
halt_max_steps: 16

H_cycles: 3
L_cycles: 6

H_layers: 0
L_layers: 2

hidden_size: 512
num_heads: 8
expansion: 4

puzzle_emb_ndim: ${.hidden_size}

pos_encodings: rope
forward_dtype: bfloat16

mlp_t: False
puzzle_emb_len: 16
no_ACT_continue: True

# ===== DQN Enhancement Parameters =====

# Enable DQN
enable_dqn: True

# Replay Buffer
dqn_buffer_capacity: 20000
dqn_buffer_min_size: 5000
dqn_batch_size: 256

# Learning Parameters
dqn_gamma: 0.99  # Discount factor

# Target Network (Soft Polyak Update)
dqn_target_tau: 0.005  # Soft update rate (0.5% online, 99.5% target)

# Exploration (Epsilon-Greedy)
dqn_epsilon_start: 0.5   # Conservative start (not 1.0)
dqn_epsilon_end: 0.05
dqn_epsilon_decay_steps: 100000

# Reward Shaping (Simplified - 3 parameters)
reward_step_penalty: 0.01          # -0.01 per step (1 step = 1% accuracy trade-off)
reward_terminal_correct: 1.0       # +1.0 bonus for correct halt, scaled by efficiency
reward_terminal_incorrect: -0.5    # -0.5 penalty for incorrect early halt

# Q-Head Architecture (Training vs Inference)
q_head_type: "rnn"  # Options: "mlp" (default), "rnn" (better temporal), "mini_attention"
q_head_hidden_size: 128  # Hidden size for RNN/attention Q-head
q_head_num_layers: 1     # Number of RNN layers or attention heads

# Memory Bank Configuration
enable_memory: True       # Enable associative memory for pattern reuse
memory_capacity: 4096     # Number of memory slots (increased from 512)
memory_num_heads: 8       # Multi-head attention for memory retrieval
memory_dropout: 0.1       # Dropout for memory attention
memory_reward_bonus: 0.5  # Reward bonus for memory-assisted improvements
memory_reward_threshold: 0.05  # Minimum improvement (5%) to store in memory

# Export Configuration (for CPU inference)
export_to_snn: False     # Convert Q-head to Spiking Neural Network
export_to_bnn: False     # Convert Q-head to Binary Neural Network (1-bit weights)

# Notes:
# - Train with dense MLP/RNN Q-head on GPU for stability
# - Export to SNN/BNN for 10-100× energy efficiency on CPU/neuromorphic hardware
# - Simplified reward: r = Δacc - 0.01*step + terminal_bonus
# - Epsilon decay: 0.5 → 0.05 over 100K steps
# - To disable DQN, set enable_dqn: False
