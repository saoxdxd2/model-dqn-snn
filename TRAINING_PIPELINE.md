# Training Pipeline Documentation

## ğŸ¯ Quick Overview

```
train.py â†’ pretrain.py â†’ Training Loop
   â†“           â†“              â†“
Dataset    Hydra Config   Model Forward/Backward
Builder    Loading        Checkpointing
```

---

## ğŸ“Š Training Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ENTRY POINT: train.py                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  1. Check for Checkpoint    â”‚
         â”‚     (auto-resume enabled)   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  2. Dataset Preparation     â”‚
         â”‚    â€¢ Streaming builder      â”‚
         â”‚    â€¢ Wait for consolidated  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  3. Call pretrain.py        â”‚
         â”‚     with Hydra config       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CORE TRAINING: pretrain.py                  â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ INITIALIZATION                                      â”‚    â”‚
â”‚  â”‚  â€¢ Load config (cfg_pretrain.yaml)                 â”‚    â”‚
â”‚  â”‚  â€¢ Setup distributed (if multi-GPU)                â”‚    â”‚
â”‚  â”‚  â€¢ Register graceful shutdown (Ctrl+C handler)     â”‚    â”‚
â”‚  â”‚  â€¢ Load/build datasets                             â”‚    â”‚
â”‚  â”‚  â€¢ Initialize model, optimizer, loss               â”‚    â”‚
â”‚  â”‚  â€¢ Setup EMA, gradient monitor, W&B logging        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                       â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ TRAINING LOOP (per epoch)                          â”‚    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â”‚  FOR each batch in train_loader:                   â”‚    â”‚
â”‚  â”‚    â”œâ”€ Forward pass                                 â”‚    â”‚
â”‚  â”‚    â”œâ”€ Compute loss (+ DQN, MTP, VQ losses)         â”‚    â”‚
â”‚  â”‚    â”œâ”€ Backward pass                                â”‚    â”‚
â”‚  â”‚    â”œâ”€ Optimizer step                               â”‚    â”‚
â”‚  â”‚    â”œâ”€ Update EMA                                   â”‚    â”‚
â”‚  â”‚    â”œâ”€ Log metrics to W&B                           â”‚    â”‚
â”‚  â”‚    â””â”€ Check shutdown signal                        â”‚    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â”‚  EVERY eval_interval epochs:                       â”‚    â”‚
â”‚  â”‚    â”œâ”€ Evaluation on test set                       â”‚    â”‚
â”‚  â”‚    â”œâ”€ Run custom evaluators (ARC, Code, etc.)      â”‚    â”‚
â”‚  â”‚    â””â”€ Save checkpoint                              â”‚    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                       â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ CHECKPOINTING                                      â”‚    â”‚
â”‚  â”‚  â€¢ Saves to: checkpoints/multimodal-hesc/latest.ptâ”‚    â”‚
â”‚  â”‚  â€¢ Includes: model, optimizer, step, epoch         â”‚    â”‚
â”‚  â”‚  â€¢ EMA model saved separately                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ How to Run Training

### Basic Training
```bash
python train.py
```

**What happens:**
1. Checks for existing checkpoint â†’ auto-resumes if found
2. Waits for `consolidated_000.pt` (streaming dataset)
3. Starts training with COCONUT latent planning enabled
4. Logs to Weights & Biases
5. Saves checkpoints periodically

### Training Options
```bash
# Force rebuild dataset
python train.py --rebuild-dataset

# Only build dataset, don't train
python train.py --dataset-only

# Disable incremental training (wait for full dataset)
python train.py --no-incremental
```

---

## ğŸ“‚ File Locations

### Checkpoints
```
checkpoints/
â”œâ”€â”€ multimodal-hesc/
â”‚   â”œâ”€â”€ latest.pt              # Main checkpoint (auto-resumes from this)
â”‚   â””â”€â”€ ema_model.pt           # EMA weights
```

### Datasets
```
datasets/
â””â”€â”€ vision_unified/
    â”œâ”€â”€ consolidated_000.pt    # First 100 encoded batches
    â”œâ”€â”€ consolidated_001.pt    # Next 100 batches
    â””â”€â”€ ...
```

### Configs
```
config/
â”œâ”€â”€ cfg_pretrain.yaml          # Main training config
â””â”€â”€ arch/
    â”œâ”€â”€ multimodal_hesc.yaml   # Model architecture (COCONUT enabled)
    â”œâ”€â”€ code_optimized.yaml
    â”œâ”€â”€ text_optimized.yaml
    â””â”€â”€ ...
```

---

## ğŸ”„ Training Loop Breakdown

### 1. **Initialization Phase**
```python
# In pretrain.py launch()
- Load Hydra config (cfg_pretrain.yaml)
- Setup distributed training (if multi-GPU)
- Register Ctrl+C handler for graceful shutdown
- Load datasets (train + test splits)
- Initialize model from config/arch/*.yaml
- Create optimizer (AdamAtan2)
- Setup EMA helper
- Initialize W&B logging
```

### 2. **Training Iteration**
```python
for epoch in range(config.epochs):
    for batch in train_loader:
        # 1. Forward pass
        carry = model.initial_carry(batch)
        new_carry, outputs = model(carry, batch)
        
        # 2. Compute losses
        loss = criterion(outputs['logits'], targets)
        + dqn_loss          # Reinforcement learning
        + vq_loss           # Vector quantization
        + mtp_loss          # Multi-token prediction
        
        # 3. Backward + optimize
        loss.backward()
        optimizer.step()
        
        # 4. Update EMA
        ema_helper.update(model)
        
        # 5. Log metrics
        wandb.log({
            'loss': loss.item(),
            'lr': current_lr,
            'step': global_step
        })
```

### 3. **Evaluation Phase** (every N epochs)
```python
model.eval()
with torch.no_grad():
    for batch in eval_loader:
        outputs = model(batch)
        metrics = compute_metrics(outputs, targets)
        
# Run custom evaluators
for evaluator in evaluators:
    evaluator.run(model, eval_loader)
```

### 4. **Checkpointing**
```python
# Save every eval_interval or on graceful shutdown
checkpoint = {
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'step': global_step,
    'epoch': current_epoch,
    'config': config
}
torch.save(checkpoint, 'checkpoints/multimodal-hesc/latest.pt')
```

---

## ğŸ›ï¸ Key Configuration Parameters

### Training Hyperparameters (cfg_pretrain.yaml)
```yaml
# Dataset
data_paths: ["datasets/vision_unified"]

# Training
global_batch_size: 32
epochs: 100
lr: 3e-4
lr_min_ratio: 0.1          # Cosine schedule min LR

# Checkpointing
checkpoint_path: "checkpoints/multimodal-hesc"
checkpoint_every_eval: true
eval_interval: 10           # Evaluate every 10 epochs

# Stability
ema: true                   # Exponential moving average
ema_decay: 0.999
gradient_clip: 1.0

# Logging
wandb_project: "trm-training"
```

### Model Architecture (config/arch/multimodal_hesc.yaml)
```yaml
# TRM Encoder
hidden_size: 1024
H_cycles: 2
L_cycles: 3

# COCONUT Latent Planning
enable_latent_planning: true
latent_num_paths: 4
latent_planning_depth: 2

# Features
enable_memory: true
enable_dqn: true
enable_mtp: true
use_vq_codebook: true
```

---

## ğŸ“Š Dataset Pipeline

### Streaming Builder Flow
```
Raw Data Sources (ARC, TinyStories, Code)
    â†“
text_renderer.py (convert text â†’ images)
    â†“
TRM Vision Encoder (encode images â†’ capsules)
    â†“
Save as batches: batch_00000.pt, batch_00001.pt, ...
    â†“
Consolidate: 100 batches â†’ consolidated_000.pt
    â†“
Training loads consolidated_*.pt files
```

### Key Files
- `dataset/streaming_builder.py` - Streaming dataset builder
- `dataset/build_multimodal_dataset.py` - Main builder class
- `dataset/base_builder.py` - Abstract base class

---

## ğŸ›¡ï¸ Graceful Shutdown

**Press Ctrl+C during training:**
```
1. Signal handler catches SIGINT
2. Sets shutdown_requested flag
3. Current batch finishes safely
4. Checkpoint saved
5. Training exits cleanly
```

**Resume training:**
```bash
python train.py  # Auto-resumes from latest.pt
```

---

## ğŸ“ˆ Monitoring Training

### Weights & Biases Dashboard
- Loss curves (total, DQN, VQ, MTP)
- Learning rate schedule
- Gradient flow statistics
- Model metrics (accuracy, perplexity)
- Hardware utilization

### Local Logs
```bash
# View recent logs
tail -f train.log

# Check checkpoint status
ls -lh checkpoints/multimodal-hesc/
```

---

## ğŸ”§ Advanced Features

### 1. **Multi-GPU Training**
```bash
torchrun --nproc_per_node=4 pretrain.py
```

### 2. **EMA (Exponential Moving Average)**
- Maintains shadow copy of model weights
- Smoother convergence
- Used during evaluation
- Config: `ema: true`, `ema_decay: 0.999`

### 3. **Gradient Monitoring**
- Tracks gradient flow through layers
- Detects vanishing/exploding gradients
- Periodic cleanup to prevent memory leaks

### 4. **DQN Buffer Management**
- Stores experiences for RL training
- Configurable capacity: `dqn_buffer_capacity: 500000`
- Warmup period: `dqn_warmstart_steps: 10000`

---

## ğŸ› Common Issues

### "RuntimeError: CUDA out of memory"
**Solutions:**
- Reduce batch size in `cfg_pretrain.yaml`
- Disable memory bank: `enable_memory: false`
- Disable COCONUT: `enable_latent_planning: false`
- Enable gradient checkpointing: `enable_gradient_checkpointing: true`

### "No consolidated files found"
**Solution:**
```bash
# Build dataset first
python train.py --dataset-only

# Wait for consolidated_000.pt to appear
ls datasets/vision_unified/
```

### Training stops without saving
**Solution:**
- Use graceful shutdown (Ctrl+C once, wait for checkpoint)
- Check disk space
- Verify checkpoint directory permissions

---

## ğŸ“ Training Pipeline Summary

**Entry Point:** `train.py`
- Auto-resume detection
- Dataset preparation
- Calls pretrain.py

**Core Training:** `pretrain.py`
- Hydra config loading
- Distributed setup
- Training loop
- Evaluation
- Checkpointing

**Dataset:** Streaming builder
- On-the-fly encoding
- Consolidated batches
- Resume support

**Model:** TRM with COCONUT
- Vision-unified architecture
- 12 capsules â†’ Recursive reasoning â†’ Latent planning â†’ Output
- 163M parameters (35M for COCONUT)

---

## ğŸš¦ Next Steps

1. **Test Setup:** `python scripts/test.py`
2. **Build Dataset:** `python train.py --dataset-only`
3. **Start Training:** `python train.py`
4. **Monitor:** Check W&B dashboard
5. **Resume:** Same command auto-resumes

For detailed architecture info, see `PIPELINE.md`.
